{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "required_packages = {\n",
    "    'pandas': 'For data manipulation and analysis',\n",
    "    'numpy': 'For numerical computing',\n",
    "    'xarray': 'For working with labeled multi-dimensional arrays',\n",
    "    'dask': 'For parallel computing capabilities',\n",
    "    'xbatcher': 'For batch processing of large datasets',\n",
    "    'matplotlib': 'For data visualization',\n",
    "    'torch': 'For deep learning',\n",
    "}\n",
    "\n",
    "# Check and install missing packages\n",
    "for package, purpose in required_packages.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✓ {package} is already installed ({purpose})\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package} ({purpose})...\")\n",
    "        \n",
    "        install_package(package)\n",
    "        print(f\"✓ {package} has been installed\")\n",
    "\n",
    "\n",
    "# Verify installations\n",
    "import os\n",
    "\n",
    "# Data Analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "## Xarray and Extensions\n",
    "import xarray as xr\n",
    "import xbatcher as xb\n",
    "import xbatcher.loaders.torch\n",
    "import dask\n",
    "\n",
    "# Deep Learning (torch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "\n",
    "import psutil\n",
    "import math\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, progress, performance_report\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\nAll required packages are installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Transfer to Xarray - DataArray and Labeling the dims and coords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls '/home/NAS/homes/isaac-10009/Data/Academic/AtmSci7115 Climate change and extreme events – machine learning hands-on/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root path\n",
    "root_path = '/home/NAS/homes/isaac-10009/Data/Academic/AtmSci7115 Climate change and extreme events – machine learning hands-on/data/topic1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Training data\n",
    "# TRAINING_NUM = 35226\n",
    "\n",
    "# # features are stored in a numpy array with shape (35226, 64, 64, 2) \n",
    "# # load the features from npz file and open\n",
    "# features_path = os.path.join(root_path, f'[7C40]features_({TRAINING_NUM}, 64, 64, 2).npz')\n",
    "\n",
    "# X_train = xr.DataArray(\n",
    "#     np.load(features_path)['arr_0'],\n",
    "#     dims=['time', 'height', 'width', 'channel'],  # Rename dimensions as needed\n",
    "#     name='train_x',\n",
    "#     coords={\n",
    "#         'time': range(TRAINING_NUM),\n",
    "#         'height': range(64),\n",
    "#         'width': range(64),\n",
    "#         'channel': ['feature0', 'feature1']  # Rename channels as needed\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # labels are stored in a numpy array with shape (35226,)\n",
    "# # load the label npz file and open\n",
    "# labels_path = os.path.join(root_path, f'[7C40]labels_({TRAINING_NUM},).npz')\n",
    "# y_train = xr.DataArray(\n",
    "#     np.load(labels_path)['arr_0'],\n",
    "#     dims=['time'],\n",
    "#     name='train_y',\n",
    "#     coords={'time': range(TRAINING_NUM)}\n",
    "# )\n",
    "\n",
    "# print(f\"X_train shape: {X_train.shape}\")\n",
    "# print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Test data\n",
    "# TESTING_NUM = 171081\n",
    "\n",
    "\n",
    "# # features are stored in a numpy array with shape (171081, 64, 64, 2) \n",
    "# # load the features from npz file and open\n",
    "# features_path = os.path.join(root_path, f'[7C40]features_({TESTING_NUM}, 64, 64, 2).npz')\n",
    "\n",
    "# X_test = xr.DataArray(\n",
    "#     np.load(features_path)['arr_0'],\n",
    "#     dims=['time', 'height', 'width', 'channel'],  # Rename dimensions as needed\n",
    "#     name='test_x',\n",
    "#     coords={\n",
    "#         'time': range(TESTING_NUM),\n",
    "#         'height': range(64),\n",
    "#         'width': range(64),\n",
    "#         'channel': ['feature0', 'feature1']  # Rename channels as needed\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # labels are stored in a numpy array with shape (171081,)\n",
    "# # load the label npz file and open\n",
    "# labels_path = os.path.join(root_path, f'[7C40]labels_({TESTING_NUM},).npz')\n",
    "# y_test = xr.DataArray(\n",
    "#     np.load(labels_path)['arr_0'],\n",
    "#     dims=['time'],\n",
    "#     name='test_y',\n",
    "#     coords={'time': range(TESTING_NUM)}\n",
    "# )\n",
    "\n",
    "# print(f\"X_test shape: {X_test.shape}\")\n",
    "# print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Versiom with Dask to faster Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OS parms\n",
    "CORE = psutil.cpu_count(logical=False)\n",
    "TOTAL_MEMORY = psutil.virtual_memory().total / (1024**3) \n",
    "\n",
    "core_per_worker = CORE - 2\n",
    "memory_limit_per_core = math.floor((TOTAL_MEMORY / CORE-1) * 0.8)\n",
    "\n",
    "# Set dask cluster\n",
    "cluster = LocalCluster(\n",
    "    n_workers=core_per_worker,\n",
    "    threads_per_worker = 4,\n",
    "    memory_limit=f'{memory_limit_per_core}GB',\n",
    "    # dashboard_address=':0',\n",
    "    dashboard_address=42295,\n",
    "    silence_logs='error',  \n",
    "    lifetime='1h',     # 設置生命週期\n",
    "    processes=False,     # 使用多進程而不是線程\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster information:\\n\")\n",
    "print(f\"Number of workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"Total threads: {client.cluster.scheduler.total_nthreads}\")\n",
    "print(f\"Total memory: {client.cluster.scheduler.total_nthreads} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training data\n",
    "TRAINING_NUM = 35226\n",
    "\n",
    "# Measure performance\n",
    "start_time = time.time()\n",
    "\n",
    "# Set chunk sizes for optimal performance\n",
    "# Chunk along time dimension for best parallel processing\n",
    "# chunks = {'time': 1000, 'height': -1, 'width': -1, 'channel': -1} \n",
    "\n",
    "# Load features with dask - use delayed loading\n",
    "print(\"Loading feature data with Dask...\")\n",
    "features_path = os.path.join(root_path, f'[7C40]features_({TRAINING_NUM}, 64, 64, 2).npz')\n",
    "# np_data = np.load(features_path)['arr_0']\n",
    "# Convert to dask array with appropriate chunking\n",
    "# dask_features = da.from_array(np_data, chunks=(5000, 64, 64, 2))\n",
    "with ProgressBar():\n",
    "    lazy_load = dask.delayed(np.load)(features_path)\n",
    "    dask_features = da.from_delayed(\n",
    "        lazy_load['arr_0'],\n",
    "        shape=(TRAINING_NUM, 64, 64, 2),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Create xarray DataArray with dask array\n",
    "    X_train = xr.DataArray(\n",
    "        dask_features,\n",
    "        dims=['time', 'height', 'width', 'channel'],\n",
    "        name='train_x',\n",
    "        coords={\n",
    "            'time': range(TRAINING_NUM),\n",
    "            'height': range(64),\n",
    "            'width': range(64),\n",
    "            'channel': ['feature0', 'feature1']\n",
    "        }\n",
    "    )\n",
    "print(f\"X_train shape: {X_train.shape}, chunks: {X_train.chunks}\")\n",
    "\n",
    "# Load labels with dask\n",
    "print(f\"\\nTrain feature loading completed in {time.time() - start_time:.2f} seconds\\nLoading Train label data with Dask...\")\n",
    "\n",
    "with ProgressBar():\n",
    "    labels_path = os.path.join(root_path, f'[7C40]labels_({TRAINING_NUM},).npz')\n",
    "    lazy_load = dask.delayed(np.load)(labels_path)\n",
    "    dask_labels = da.from_delayed(\n",
    "        lazy_load['arr_0'],\n",
    "        shape=(TRAINING_NUM,),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Create xarray DataArray with dask array\n",
    "    y_train = xr.DataArray(\n",
    "        dask_labels,\n",
    "        dims=['time'],\n",
    "        name='train_y',\n",
    "        coords={\n",
    "            'time': range(TRAINING_NUM),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Print information about the data\n",
    "print(f\"y_train shape: {y_train.shape}, chunks: {y_train.chunks}\")\n",
    "# y_train.compute()\n",
    "\n",
    "# Report loading time\n",
    "print(f\"\\nTrain Data loading completed in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train.data))  # if dask.array.Array that use Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test data\n",
    "TESTING_NUM = 171081\n",
    "\n",
    "# Measure performance\n",
    "start_time = time.time()\n",
    "\n",
    "# features are stored in a numpy array with shape (171081, 64, 64, 2) \n",
    "# Load features with dask - use delayed loading\n",
    "print(\"Loading feature data with Dask...\")\n",
    "features_path = os.path.join(root_path, f'[7C40]features_({TESTING_NUM}, 64, 64, 2).npz')\n",
    "# np_data = np.load(features_path)['arr_0']\n",
    "# Convert to dask array with appropriate chunking\n",
    "# dask_features = da.from_array(np_data, chunks=(1000, 64, 64, 2))\n",
    "\n",
    "\n",
    "with ProgressBar():\n",
    "    lazy_load = dask.delayed(np.load)(features_path)\n",
    "    dask_features = da.from_delayed(\n",
    "        lazy_load['arr_0'],\n",
    "        shape=(TESTING_NUM, 64, 64, 2),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    X_test = xr.DataArray(\n",
    "        dask_features,\n",
    "        dims=['time', 'height', 'width', 'channel'],  # Rename dimensions as needed\n",
    "        name='test_x',\n",
    "        coords={\n",
    "            'time': range(TESTING_NUM),\n",
    "            'height': range(64),\n",
    "            'width': range(64),\n",
    "            'channel': ['feature0', 'feature1']  # Rename channels as needed\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}, chunks: {X_test.chunks}\")\n",
    "\n",
    "# labels are stored in a numpy array with shape (171081,)\n",
    "# Load labels with dask\n",
    "print(f\"\\nTest feature loading completed in {time.time() - start_time:.2f} seconds\\nLoading Test label data with Dask...\\n\")\n",
    "labels_path = os.path.join(root_path, f'[7C40]labels_({TESTING_NUM},).npz')\n",
    "\n",
    "\n",
    "with ProgressBar():\n",
    "    lazy_load = dask.delayed(np.load)(labels_path)\n",
    "    dask_labels = da.from_delayed(\n",
    "        lazy_load['arr_0'],\n",
    "        shape=(TESTING_NUM,),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    y_test = xr.DataArray(\n",
    "        dask_labels,\n",
    "        dims=['time'],\n",
    "        name='test_y',\n",
    "        coords={'time': range(TESTING_NUM)}\n",
    "    )\n",
    "\n",
    "print(f\"y_test shape: {y_test.shape}, chunks: {y_test.chunks}\")\n",
    "\n",
    "print(f\"\\nTest Data loading completed in {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isel(time=0).sel(channel='feature1').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Concat to Xarray - Dataset that makes it mange easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset(\n",
    "    data_vars={\n",
    "        'features': X_train,  \n",
    "        'labels': y_train    \n",
    "    }\n",
    ").transpose('time', 'channel', 'height', 'width')  # Transpose dimensions to Pytorch format\n",
    "\n",
    "# ds.persist() # Persist data in memory for faster access\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = xr.Dataset(\n",
    "    data_vars={\n",
    "        'features': X_test,  \n",
    "        'labels': y_test,    \n",
    "    }\n",
    ").transpose('time', 'channel', 'height', 'width') #(B, C, H, W) torch format\n",
    "\n",
    "ds_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Use Xbatcher to batch generator from xarray data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the size\n",
    "n_timepoint_in_each_sample = 1\n",
    "n_timepoint_in_each_batch = 32\n",
    "# input_overlap = 3 # overlap 3 time points 1-3 2-4 3-5 4-6\n",
    "\n",
    "# Define batch generators\n",
    "X_bgen = xb.BatchGenerator(\n",
    "    ds['features'],\n",
    "    input_dims = {'height':64, 'width':64, 'channel':2}, # don't put time dimension here (one time)\n",
    "    batch_dims = {'time': n_timepoint_in_each_batch}, # batch size dims (name) can't be same to input dims\n",
    "    preload_batch=True,\n",
    ")\n",
    "\n",
    "y_bgen = xb.BatchGenerator(\n",
    "    ds['labels'], \n",
    "    input_dims={}, \n",
    "    batch_dims={'time': n_timepoint_in_each_batch}, \n",
    "    preload_batch=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test first one batch generator\n",
    "X_bgen[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first three batch size shape\n",
    "for i, (batch_X, batch_y) in enumerate(zip(X_bgen, y_bgen)):\n",
    "\n",
    "    print(f\"Batch {i} shapes: X={batch_X.shape}, y={batch_y.shape}\")\n",
    "    if i >= 2:  \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of batches: {len(X_bgen)}\")\n",
    "print(f\"First train batch shape: {X_bgen[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Transfer to DataLoader from xb.batch.generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map batches to a PyTorch-compatible dataset\n",
    "\n",
    "## Train dataset (Ready to transfer to PyTorch.Dataloader)\n",
    "train_dataset = xbatcher.loaders.torch.MapDataset(X_bgen, y_bgen)\n",
    "\n",
    "# set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# cal val size\n",
    "val_size = 0.2  # use 20% of the data for validation\n",
    "n_samples = len(train_dataset)\n",
    "n_val = int(n_samples * val_size)\n",
    "n_train = n_samples - n_val\n",
    "\n",
    "# split the dataset \n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, \n",
    "    [n_train, n_val]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset (same the train_dataset: we neet to transfer to torch)\n",
    "X_bgen_test = xb.BatchGenerator(\n",
    "    ds_test['features'],\n",
    "    input_dims = {'height':64, 'width':64, 'channel':2}, # don't put time dimension here (one time)\n",
    "    batch_dims = {'time': 4096}, # batch size dims (name) can't be same to input dims (4096 is 2^12 for better performance) \n",
    "    preload_batch=True,\n",
    ")\n",
    "\n",
    "y_bgen_test = xb.BatchGenerator(\n",
    "    ds_test['labels'], \n",
    "    input_dims={}, \n",
    "    batch_dims={'time': 4096}, \n",
    "    preload_batch=True\n",
    ")\n",
    "\n",
    "test_dataset = xbatcher.loaders.torch.MapDataset(X_bgen_test, y_bgen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=None, \n",
    "    shuffle=True,   # shuffle the data\n",
    "    \n",
    "    prefetch_factor=3,  # Prefetch up to 3 batches in advance to reduce data loading latency\n",
    "    num_workers=4,  # Use 4 parallel worker processes to load data concurrently\n",
    "    persistent_workers=True,  # Keep workers alive between epochs for faster subsequent epochs\n",
    "    multiprocessing_context='forkserver',  # Use \"forkserver\" to spawn subprocesses, ensuring stability in multiprocessing\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=None,\n",
    "    shuffle=False,  # validation data should not be shuffled\n",
    "\n",
    "    prefetch_factor=3,  # Prefetch up to 3 batches in advance to reduce data loading latency\n",
    "    num_workers=4,  # Use 4 parallel worker processes to load data concurrently\n",
    "    persistent_workers=True,  # Keep workers alive between epochs for faster subsequent epochs\n",
    "    multiprocessing_context='forkserver',  # Use \"forkserver\" to spawn subprocesses, ensuring stability in multiprocessing\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=None,\n",
    "    shuffle=False,  # validation data should not be shuffled\n",
    "\n",
    "    prefetch_factor=3,  # Prefetch up to 3 batches in advance to reduce data loading latency\n",
    "    num_workers=4,  # Use 4 parallel worker processes to load data concurrently\n",
    "    persistent_workers=True,  # Keep workers alive between epochs for faster subsequent epochs\n",
    "    multiprocessing_context='forkserver',  # Use \"forkserver\" to spawn subprocesses, ensuring stability in multiprocessing\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "print(f'First check Feature batch shape: {train_features.size()}, type: {train_features.dtype}')\n",
    "print(f'First check Labels batch shape: {train_labels.size()}, type: {train_labels.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features, val_labels = next(iter(val_dataloader))\n",
    "\n",
    "print(f'First check Feature batch shape: {val_features.size()}, type: {val_features.dtype}')\n",
    "print(f'First check Labels batch shape: {val_labels.size()}, type: {val_labels.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = next(iter(test_dataloader))\n",
    "\n",
    "print(f'First check Feature batch shape: {test_features.size()}, type: {test_features.dtype}')\n",
    "print(f'First check Labels batch shape: {test_labels.size()}, type: {test_labels.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "try:\n",
    "    wandb.finish()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='image-classification',  # 替換成您的項目名稱\n",
    "    name='binary-classification-1',    # 替換成您的實驗名稱\n",
    "    log_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
    "import torchvision.models as models\n",
    "\n",
    "class SimpleClassifier(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # 使用預訓練的 ResNet18\n",
    "        self.model = models.resnet18(weights=None)\n",
    "        self.model.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False) \n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 2) # output class 2 (binary classification)\n",
    "         \n",
    "        # init metrics\n",
    "        self.train_acc = Accuracy(task='binary')\n",
    "        self.val_acc = Accuracy(task='binary')\n",
    "        self.test_acc = Accuracy(task='binary')\n",
    "        self.test_precision = Precision(task='binary')\n",
    "        self.test_recall = Recall(task='binary')\n",
    "        self.test_f1 = F1Score(task='binary')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat.squeeze(), y.float())\n",
    "        acc = self.train_acc(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat.squeeze(), y.float())\n",
    "        acc = self.val_acc(y_hat.squeeze(), y)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.1, patience=10\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set checkpoint callback and save path\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../log/checkpoints/',\n",
    "    filename='model-{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator='gpu',\n",
    "    devices=1, \n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassifier()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試模型\n",
    "trainer.test(dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally close the dask cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8087, -0.3754, -0.3517,  0.9705, -0.3876],\n",
       "        [-1.2446,  0.6229, -1.4622,  0.2435,  0.3064],\n",
       "        [ 0.2169, -1.5220, -1.1998, -0.7979, -0.8019]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8683, -0.3490,  0.2707,  0.8888, -1.4920],\n",
       "        [ 1.3720,  0.2165, -1.1045,  0.7531, -2.0253],\n",
       "        [-0.3294, -0.0216, -0.6716, -0.0613,  0.2951]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1091, 0.0396, 0.6580, 0.0519, 0.1414],\n",
       "        [0.5075, 0.1341, 0.0324, 0.2645, 0.0615],\n",
       "        [0.6403, 0.1165, 0.0357, 0.1892, 0.0182]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyClimaDist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
